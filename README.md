#### Install env
1) create venv named `myenv`
```bash
python -m venv myenv
```
2) activate venv
```bash
myenv\Scripts\activate    // for Window
source myenv/bin/activate // for Linux
```

#### install dependencies
```bash
pip install -r scan_service/requirements.txt
```
#### run example
1) setup modal for remote compute (skip if already setup)
    - create account at [modal.com](https://modal.com/)
    - run setup
```bash
modal setup
```
2) deploy inference server
```bash
modal deploy inference_server/run_with_vllm.py
```
output from terminal may look like this:
```bash
 Created objects.
â”œâ”€â”€ ðŸ”¨ Created mount 
â”‚   ./inference_server/run_with_vllm.py
â”œâ”€â”€ ðŸ”¨ Created mount ./commons
â””â”€â”€ ðŸ”¨ Created web function serve => 
    https://<account>--inference-server-internvl3-serve.modal.run
âœ“ App deployed in 3.831s! ðŸŽ‰
```
next,
**COPY** the url after 'Created web function serve =>'
open *commons/configs/model.py*, at line 21, **PASTE** the url

3) run scan_service
```bash
python scan_service/app.py --file_path QD-331-TW.pdf
```
where *QD-331-TW.pdf* is a example file
result will be available in terminal output
#### WARNING
- Fist time **run scan_service** may take time, due to vllm

#### Project structure
```
.
â”œâ”€â”€ commons/                    # shared package
â”‚   â”œâ”€â”€ configs
â”‚   â”‚   â””â”€â”€ model.py
â”‚   â”‚   
â”‚   â”œâ”€â”€ schemas/
â”‚   â”‚   â”œâ”€â”€ model.py
â”‚   â”‚   â””â”€â”€ api.py
â”‚   â”‚   
â”‚   â””â”€â”€ logger.py
â”‚
â”œâ”€â”€ inference_server/           # for host LLM model
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ run_with_vllm.py
â”‚
â”œâ”€â”€ scan_service/               # service worker
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â”œâ”€â”€ examples.py
â”‚   â”‚   â””â”€â”€ infer.py
â”‚   â”‚
â”‚   â”œâ”€â”€ utils.py
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ main_app/                   # main service
|   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ requirements.txt
|
â””â”€â”€ README.md
```

